# Obsidian Vault Configuration
vault:
  exclude_folders: ["templates", ".trash", ".git"]
  file_extensions: [".md"]

# Storage Configuration
storage:
  # provider can be 'local' or 'gcs'
  provider: ${STORAGE_PROVIDER:-local} # Default to local if not set
  local:
    # Used if provider is 'local'
    vault_path: ${LOCAL_VAULT_PATH}
  gcs:
    # Used if provider is 'gcs'
    bucket_name: ${GCS_BUCKET_NAME:-}
    credentials_path: ${GCS_CREDENTIALS_PATH:-} # Optional, for local dev

# Embedding Configuration
embeddings:
  provider: openai  # "openai" or "local"
  model: text-embedding-3-small  # Fast and cost-effective
  local_model: all-mpnet-base-v2  # for local embeddings
  batch_size: 32
  max_tokens: 8191

# Vector Store Configuration
vector_store:
  type: "faiss"  # or "chroma" or "qdrant"
  dimension: 1536  # 1536 for OpenAI text-embedding-3-small
  index_path: data/vector_store
  similarity_metric: "cosine"

# Text Processing
chunking:
  chunk_size: 512
  chunk_overlap: 50
  split_by: "paragraph"  # or "sentence"

# Retrieval Configuration
retrieval:
  k: 15  # Number of documents to retrieve for comprehensive responses
  max_graph_distance: 2  # Maximum graph traversal distance

# Graph Retriever Configuration (for enhanced_graph_retriever.py)
graph_retriever:
  entry_points:
    vector_entry_count: 3
    tag_entry_enabled: true
    entry_weight_vector: 0.7
    entry_weight_tags: 0.3
  traversal:
    max_hops: 2
    max_documents: 10
    tag_expansion_enabled: true
    path_expansion_enabled: true
    min_similarity: 0.5
  hybrid:
    enabled: true
    graph_weight: 0.6
    vector_weight: 0.4
    recency_bonus: 0.1

# OpenAI Chat Model Configuration
chat_model:
  name: gpt-4o                     # For higher quality responses
  # name: gpt-4o-mini             # For cost efficiency
  temperature: 0.7
  max_response_tokens: 16384       # GPT-4o max response tokens
  max_context_tokens: 128000       # Total context window (auto-detected if not set)
  system_prompt: ${OPENAI_SYSTEM_PROMPT}

# API Configuration
api:
  host: 127.0.0.1
  port: 8000
  debug: false

# Logging
logging:
  level: "INFO"
  file: "logs/gnosis.log"

# Directory Settings
directories:
  docs_dir: docs
  data_dir: "data"

# Fast Indexing Configuration - Optimized for OpenAI + Massive Vault
fast_indexing:
  # Use aggressive preset for maximum speed with OpenAI API
  preset: large_vault  # Balanced speed and reliability
  
  # Manual overrides for your massive vault (uncomment to use)
  # batch_size: 100                    # Smaller batches to avoid rate limits
  # max_concurrent_requests: 8         # Conservative concurrency
  # checkpoint_interval: 250           # More frequent checkpoints
  # embedding_timeout: 120             # Longer timeout for large batches
  # use_streaming: true                # Memory-efficient processing
  # max_memory_mb: 3000               # Memory limit in MB
  
  # Advanced settings
  embedding_model: text-embedding-3-small  # Fast and affordable
  retry_attempts: 3
  delay_between_batches: 0.5  # Small delay to avoid rate limits
  
  # Memory management
  cleanup_checkpoints: true  # Clean old checkpoints after successful completion
  max_checkpoint_files: 5    # Keep only last 5 checkpoint files

